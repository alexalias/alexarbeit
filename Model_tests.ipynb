{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import model_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############################### STATIC DATA #################################\n",
    "\n",
    "# List of phonemes approximated by our models. \n",
    "# As there are also other phonetic items segmented in our database, we use this list to sort those out.\n",
    "# Example of excluded phonetic items: <p:>, <usb>\n",
    "valid_phonemes = [\"a\", \"a~\", \"e\", \"E\", \"I\", \"i\", \"O\", \"o\", \"U\", \"u\", \"Y\", \"y\", \"9\", \"2\", \"a:\", \"a~:\", \"e:\", \"E:\", \"i:\",\n",
    "                 \"o:\", \"u:\", \"y:\", \"2:\", \"OY\", \"aU\", \"aI\", \"@\", \"6\", \"z\", \"S\", \"Z\", \"C\", \"x\", \"N\", \"Q\", \"b\", \"d\", \"f\", \n",
    "                 \"g\", \"h\", \"j\", \"k\", \"l\", \"m\", \"n\", \"p\", \"r\", \"s\", \"t\", \"v\"]\n",
    "\n",
    "vowel_list = [\"Q\", \"a\", \"a~\", \"e\", \"E\", \"I\", \"i\", \"O\", \"o\", \"U\", \"u\", \"Y\", \"y\", \"9\", \"2\", \"@\", \"6\", \"a:\", \"a~:\", \"e:\", \"E:\", \"i:\", \"o:\", \"u:\", \"y:\", \"2:\", \"OY\", \"aU\", \"aI\"]\n",
    "\n",
    "# Classification of phonemes used by model_wl. Note that this classification is done manually.\n",
    "# A clustering method using k-means would be more appropriate\n",
    "phon_class_dict = {\"diphthong\" : [\"aI\", \"aU\", \"OY\"], \"long_vowels\" : [\"a:\", \"E:\", \"e:\", \"i:\", \"2:\", \"@\", \"m\", \"k\", \"N\"], \n",
    "\t\t\t\t\t\"short_vowels\" : [\"a\", \"u:\", \"o:\", \"e\", \"O\", \"E\", \"C\", \"6\", \"U\", \"f\", \"y\", \"o\", \"S\", \"j\", \"y:\"], \n",
    "\t\t\t\t\t\"cons_allg\" : [\"x\", \"h\", \"l\", \"n\", \"I\", \"9\", \"z\", \"s\", \"Y\", \"v\", \"t\"], \"short_cons\" : [\"p\", \"b\", \"d\", \"g\"], \n",
    "\t\t\t\t\t\"others\" : [\"Q\", \"r\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create list of filepaths to explore. This one uses entire (training) data set, for exploration purposes.\n",
    "#path_list_training = get_path_list('C:/Users/alexutza_a/Abschlussarbeit/DB_Verbmobil/Evaluation/Training')\n",
    "\n",
    "# List of paths to the files in our test data, to later iterate on\n",
    "#path_list_test = get_path_list('C:/Users/alexutza_a/Abschlussarbeit/DB_Verbmobil/Evaluation/Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################## FUNCTIONS USING ->TRAINING DATA<- #########################################\n",
    "\n",
    "# Processed version of the phon_wordleng_dict (defined in module model_utilities): \n",
    "#    - the value-lists of the inner dict include only the phoneme proportions for the given word length (inner key)\n",
    "# Returns: a dictionary. Looks like: \n",
    "# {\"a\" : { 1 : [median_a1, mean_a1, (m_a1 + m_a1) /2], 2 : [median_a2, mean_a2, (m_a2 + m_a2) /2], ...}, \n",
    "#  \"b\" : { 1 : [...], ...}, ...}\n",
    "def phon_wl_compressed_dict(phon_wordleng_dict):\n",
    "\tphon_wl_compressed_dict = dict( (i, defaultdict(list)) for i in phon_wordleng_dict.keys() )\n",
    "\tpho_key_list = [key for key, val in phon_wordleng_dict.items()]\n",
    "\n",
    "\tfor phon in pho_key_list:\n",
    "\t\tfor w_leng in phon_wordleng_dict[phon].keys():\n",
    "\t\t\tphon_wl_compressed_dict[phon][w_leng] = [round(np.median(phon_wordleng_dict[phon][w_leng][2::3]), 3)]\n",
    "\t\t\tphon_wl_compressed_dict[phon][w_leng].append(round(np.mean(phon_wordleng_dict[phon][w_leng][2::3]), 3))\n",
    "\t\t\tphon_wl_compressed_dict[phon][w_leng].append(round((np.median(phon_wordleng_dict[phon][w_leng][2::3]) + np.mean(phon_wordleng_dict[phon][w_leng][2::3]))/2, 3))\n",
    "\n",
    "\treturn phon_wl_compressed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################## FUNCTIONS USING ->TEST DATA<- #########################################\n",
    "\n",
    "# Wendet den phon_wl_compressed_dict auf dem vorgegebenen Wort ein\n",
    "# Return: dictionary mit den entsprechenden phoneme-steaks fÃ¼r das aktuelle Wort (erfasst in composition_dict)\n",
    "#\n",
    "# Looks like: phon_wl_compressed_dict but is not nested anymore: {\"a\" : [median1, mean1, (m1+m2)/2 ], \"aU\" : [...], ...}\n",
    "#      and contains as keys only phonemes from the given word (only keys of composition_dict)\n",
    "def phoneme_steak(composition_dict, word_dur, phon_count, path_list_training):\n",
    "\tphoneme_steak_dict = dict( (i, []) for i in composition_dict.keys())\n",
    "\tp_wl_compressed_dict = phon_wl_compressed_dict(model_utilities.phon_wordleng_dict(path_list_training))\n",
    "\n",
    "\tfor phoneme in composition_dict.keys():\n",
    "\t\t# In case the word contains 2 identical phonemes, the values of the 1st one will be overwritten\n",
    "\t\t# -> this is ok, because the values would be the same (e.g. steak of \"a\" in a 5-elem-word)\n",
    "\t\tif len(p_wl_compressed_dict[phoneme][len(composition_dict)]) > 0:\n",
    "\t\t\tphoneme_steak_dict[phoneme] = p_wl_compressed_dict[phoneme][len(composition_dict)]\n",
    "\t\t# For unknown phonemes / w_lengs we calculate the steak as if all phoneme durations in word would be equal\n",
    "\t\telse:\n",
    "\t\t\tphoneme_steak_dict[phoneme] += ([word_dur/(word_dur * phon_count), word_dur/(word_dur * phon_count), word_dur/(word_dur * phon_count)])\n",
    "\n",
    "\treturn phoneme_steak_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Return: A dictionary of word composition. \n",
    "# Looks like:  {\"g\" : 1, \"@\" : 1, \"n\": 1, \"aU\" : 1}\n",
    "def build_composition_dict(datei, word_no):\n",
    "\twork_file = open(datei)\n",
    "\n",
    "\t# Initialize composition_dict with value_type : int\n",
    "\tcomposition_dict = defaultdict(int)\n",
    "\n",
    "\tfor line in work_file:\n",
    "\t\tif re.match(\"MAU\", line) and (int(line.split()[3]) == word_no):\n",
    "\t\t\tcomposition_dict[str(line.split()[4])] += 1\n",
    "\twork_file.close()\n",
    "\n",
    "\treturn composition_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns: (type: int) the predicted duration (samples) of given phoneme based on word composition\n",
    "# @param model: 0, 1, or 2 meaning (median, mean, or (median + mean)/2)\n",
    "def pdur_prediction_value(datei, word_no, phoneme, model, path_list_training):\n",
    "\tcomposition_dict = build_composition_dict(datei, word_no)\n",
    "\tword_dur, phoneme_count, mau_syl_count = model_utilities.word_statistics(datei, word_no)\n",
    "\tprint(\"It goes to pdur_pred_val\")\n",
    "\t# Actually calculate the phon duration prediction\n",
    "\tpdur_prediction = int(round( ((word_dur * phoneme_steak(composition_dict, word_dur, phoneme_count, path_list_training)[phoneme][model])/composition_dict[phoneme]), 0))\n",
    "\n",
    "\treturn pdur_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################### MODEL EVALUATION ####################################################\n",
    "\n",
    "# Model type uses individual phonemes and their median, mean, or both\n",
    "# Actually creates the lists of predictions and actual values to evaluate model\n",
    "# @param model: 0, 1, or 2 meaning (median, mean, or (median + mean)/2)\n",
    "# Return: 2 lists of integers\n",
    "def create_modelWLIP_lists(model, path_list_test, path_list_training):\n",
    "\tpredictions_list, actuals_list = [], []\n",
    "\tprint(\"Yey!\")\n",
    "\tfor datei in path_list_test:\n",
    "\t\twork_file = open(datei)\n",
    "\t\tfor line in work_file:\n",
    "\t\t\t# Restrain model to relevant phonemes\n",
    "\t\t\tif re.match(\"MAU\", line) and (str(line.split()[4]) in valid_phonemes):\n",
    "\t\t\t\tpredicted_pdur = model_wl_ip.pdur_prediction_value(datei, int(line.split()[3]), str(line.split()[4]), model, path_list_training)\n",
    "\t\t\t\tpredictions_list.append(predicted_pdur)\n",
    "\t\t\t\tactuals_list.append(int(line.split()[2]))\n",
    "\t\twork_file.close()\n",
    "\treturn predictions_list, actuals_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import model_utilities\n",
    "import performance_measures\n",
    "import create_eval_lists\n",
    "\n",
    "# Split dataset into 10 disjunct sets\n",
    "def ten_fold(dataset_path):\n",
    "    dataset = model_utilities.get_path_list(dataset_path)\n",
    "    num_folds = 10\n",
    "    subset_size = int(len(dataset)/num_folds)\n",
    "    for i in range(num_folds):\n",
    "        path_list_test = dataset[i*subset_size:][:subset_size]\n",
    "        path_list_training = dataset[:i*subset_size] + dataset[(i+1)*subset_size:]\n",
    "        \n",
    "        print(\"Starting test with fold no. \" + str(i))\n",
    "        # Using the fold-results to learn and evaluate model\n",
    "        prediction_list = create_eval_lists.create_modelWLIP_lists(1, path_list_test, path_list_training)[0] \n",
    "        actuals_list = create_eval_lists.create_modelWLIP_lists(1, path_list_test, path_list_training)[1]\n",
    "        print(\"continue\")\n",
    "        global_rmse, mae = performance_measures.calc_error_for_data(prediction_list, actuals_list)\n",
    "        corrCoef = performance_measures.corrCoef(prediction_list, actuals_list)[0]\n",
    "        print(\"Finishing test with fold no. \" + str(i))\n",
    "        print(global_rmse, mae, corrCoef)\n",
    "    return corrCoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test with fold no. 0\n"
     ]
    }
   ],
   "source": [
    "ten_fold('C:/Users/alexutza_a/Abschlussarbeit/DB_Verbmobil/Evaluation/Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
